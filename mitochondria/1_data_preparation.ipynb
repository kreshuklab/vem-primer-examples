{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332f503a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kreshuklab/vem-primer-examples/blob/main/mitochondria/1_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54df520",
   "metadata": {},
   "source": [
    " # vEM-Mitochondria: Data Preparation\n",
    " \n",
    " Lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5767cd",
   "metadata": {},
   "source": [
    "## Google Colab\n",
    "\n",
    "IMPORTANT: Run the next cells until `Download MitoEM` only if you execute this notebook on Google Colab. If you run this notebook locally, you need to set up a python environment with the correct dependencies beforehand, check out [these instructions](https://github.com/kreshuklab/vem-primer-examples#setting-up-conda-environments-advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO install dependencies with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2521f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847ff20",
   "metadata": {},
   "source": [
    "## Download MitoEM data and create volume\n",
    "\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb64da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required for downloading and volume creation\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "from glob import glob\n",
    "from shutil import copyfileobj\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import imageio\n",
    "import numpy as np\n",
    "import zarr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4f15fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url and checksum for the data\n",
    "mitoem_url = \"https://www.dropbox.com/s/z41qtu4y735j95e/EM30-H-im.zip?dl=1\"\n",
    "checksum = \"98fe259f36a7d8d43f99981b7a0ef8cdeba2ce2615ff91595f428ae57207a041\"\n",
    "tmp_path = \"./mito-em-tmp\"\n",
    "os.makedirs(data_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cdb75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU ARE RUNNING LOCALLY\n",
    "\n",
    "# this is where we save the data:\n",
    "# the complete volume\n",
    "full_output_path = \"vem-primer-data/MitoEM-H.ome.zarr\"\n",
    "# a crop of the volume that we can use to speed up downstream analysis\n",
    "cropped_output_path = \"vem-primer-data/MitoEM-H-cropped.ome.zarr\"\n",
    "\n",
    "os.makedirs(os.path.split(full_output_path)[0], exist_ok=True)\n",
    "os.makedirs(os.path.split(cropped_output_path)[0], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU ARE USING COLAB \n",
    "\n",
    "# this is where we save the data:\n",
    "# the complete volume\n",
    "# full_output_path = \"/content/gdrive/MyDrive/vem-primer-data\"\n",
    "full_output_path = \"vem-primer-data/MitoEM-H.ome.zarr\"\n",
    "# \n",
    "cropped_output_path = \"/content/gdrive/MyDrive/vem-primer-data/MitoEM-H-cropped.ome.zarr\"\n",
    "\n",
    "os.makedirs(os.path.split(full_output_path)[0], exist_ok=True)\n",
    "os.makedirs(os.path.split(cropped_output_path)[0], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d41e4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the data is already fully downloaded and initial volume has been created\n",
    "# if yes, we skip the following cells\n",
    "have_mitoem_vol = os.path.exists(full_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6af6addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "zip_path = os.path.join(tmp_path, \"MitoEM-H.zip\")\n",
    "if not have_mitoem_vol and not os.path.exists(zip_path):\n",
    "    with requests.get(mitoem_url, stream=True) as r:\n",
    "        filesize = int(r.headers.get(\"Content-Length\", 0))\n",
    "        desc = f\"Download {mitoem_url} to {zip_path}\"\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=filesize, desc=desc) as r_raw, open(zip_path, \"wb\") as f:\n",
    "            copyfileobj(r_raw, f)\n",
    "    with open(zip_path, \"rb\") as f:\n",
    "        this_checksum = hashlib.sha256(f.read()).hexdigest()\n",
    "    if this_checksum == checksum:\n",
    "        print(\"Download to\", zip_path, \"was successful.\")\n",
    "    else:\n",
    "        print(\"The file was downloaded to\", zip_path, \"but the file is likely corrupted!\")\n",
    "        print(\"Please remove\", zip_path, \"and try the download again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8b50b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the data\n",
    "unzipped_path = os.path.join(tmp_path, \"im\")\n",
    "if not have_mitoem_vol and not os.path.exists(unzipped_path):\n",
    "    with ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(path=tmp_path)\n",
    "\n",
    "# the file paths for all the pngs with image data\n",
    "image_paths = glob(os.path.join(unzipped_path, \"*.png\"))\n",
    "image_paths.sort()\n",
    "# we only write the last 500 image tiles, which correspond to the MitoEM test data-set\n",
    "image_paths = image_paths[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "592b6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data into a ome.zarr file\n",
    "# TODO actually use ome.zarr instead of normal zarr\n",
    "# FIXME this is much more inefficient than in-memory, need a better solution\n",
    "if not have_mitoem_vol:\n",
    "    # we use dask to copy the data into the ome.zarr array lazily\n",
    "    # see https://docs.dask.org/en/stable/array-creation.html\n",
    "    # https://docs.dask.org/en/latest/generated/dask.array.to_zarr.html\n",
    "    imread = dask.delayed(imageio.imread, pure=True)\n",
    "    print(\"Creating ome.zarr volume from\", len(image_paths), \"slices\")\n",
    "    images = [imread(path) for path in image_paths]\n",
    "    \n",
    "    # find the shape and datatype from the first image\n",
    "    sample = images[0].compute()\n",
    "\n",
    "    # load individual images to get a dask array for each image and stack to get a volume\n",
    "    arrays = [da.from_delayed(im, dtype=sample.dtype, shape=sample.shape)\n",
    "              for im in images]\n",
    "    volume = da.stack(arrays, axis=0)\n",
    "    chunks = (16, 128, 128)\n",
    "    volume.rechunk(chunks)\n",
    "\n",
    "    # write to zarr\n",
    "    da.to_zarr(volume, os.path.join(full_output_path, \"s0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e997cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [02:07<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write volume to ./mito-em-data/MitoEM-H.ome.zarr\n"
     ]
    }
   ],
   "source": [
    "# copy the data into a ome.zarr file (in memory; implement an efficient version with dask instead)\n",
    "# TODO actually use ome.zarr instead of normal zarr\n",
    "if not have_mitoem_vol:\n",
    "    import z5py\n",
    "    from tqdm import trange\n",
    "    \n",
    "    im0 = imageio.imread(image_paths[0])\n",
    "    shape = (len(image_paths),) + im0.shape\n",
    "    volume = np.zeros(shape, dtype=im0.dtype)\n",
    "    volume[0] = im0\n",
    "    \n",
    "    for z in trange(1, len(image_paths)):\n",
    "        volume[z] = imageio.imread(image_paths[z])\n",
    "    \n",
    "    print(\"Write volume to\", full_output_path)\n",
    "    chunks = (16, 128, 128)\n",
    "    with z5py.File(full_output_path, \"a\") as f:\n",
    "        f.create_dataset(\"s0\", data=volume, compression=\"gzip\", n_threads=4, chunks=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d2b33",
   "metadata": {},
   "source": [
    "## Downscale and crop the data\n",
    "\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ead2e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use native ome.zarr code as well\n",
    "# make scale pyramid\n",
    "from numcodecs import GZip\n",
    "scale_factors = [[1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
    "store = zarr.DirectoryStore(full_output_path)\n",
    "arrays, pyramid = [], []\n",
    "current_factor = np.array([1, 1, 1])\n",
    "with zarr.open(store, mode=\"a\") as f:\n",
    "    data = da.from_zarr(f[\"s0\"])\n",
    "    for scale, factor in enumerate(scale_factors, 1):\n",
    "        current_factor *= np.array(factor)\n",
    "        factor_dict = {k: v for k, v in enumerate(current_factor)}\n",
    "        pyramid.append(\n",
    "            da.coarsen(np.mean, data, factor_dict, trim_excess=True).rechunk(chunks)\n",
    "        )\n",
    "        arrays.append(\n",
    "            f.zeros(name=f\"s{scale}\", shape=pyramid[-1].shape, chunks=chunks, compressor=GZip())\n",
    "        )\n",
    "    da.store(pyramid, arrays, lock=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a central crop\n",
    "store_cropped = zarr.DirectoryStore(cropped_output_path)\n",
    "crop_shape = (20, 1024, 1024)\n",
    "crop = tuple(slice(sh // 2 - csh // 2, sh // 2 + csh // 2) for sh, csh in zip(data.shape, crop_shape))\n",
    "with zarr.open(store) as f_in, zarr.open(store_cropped, mode=\"a\") as f_out:\n",
    "    f_out.create_dataset(name=\"s0\", data=f_in[\"s0\"][crop], chunks=chunks, compression=GZip())\n",
    "    for scale, factor in enumerate(scale_factors, 1):\n",
    "        crop = tuple(slice(c.start // sf, c.stop // sf) for c, sf in zip(crop, factor))\n",
    "        f_out.create_dataset(name=f\"s{scale}\", data=f_in[f\"s{scale}\"][crop], chunks=chunks, compression=GZip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "designed-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conversion to other formats (if necessary):\n",
    "# export crops for the different resolutions to tifs\n",
    "with zarr.open(store_cropped, \"r\") as f:\n",
    "    for scale in range(len(f)):\n",
    "        data = f[f\"s{scale}\"][:]\n",
    "        imageio.volsave(f\"mito-em-cropped-s{scale}.tif\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
