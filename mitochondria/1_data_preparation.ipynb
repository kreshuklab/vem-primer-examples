{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54df520",
   "metadata": {},
   "source": [
    " # vEM-Mitochondria: Data Preparation\n",
    " \n",
    " Lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5767cd",
   "metadata": {},
   "source": [
    "## Google Colab\n",
    "\n",
    "IMPORTANT: Run the next cells until `Download MitoEM` only if you execute this notebook on Google Colab. If you run this notebook locally, you need to set up a python environment with the correct dependencies beforehand, check out [these instructions](https://github.com/kreshuklab/vem-primer-examples#setting-up-conda-environments-advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2521f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mount google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847ff20",
   "metadata": {},
   "source": [
    "## Download MitoEM data and create volume\n",
    "\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb64da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required for downloading and volume creation\n",
    "import hashlib\n",
    "import os\n",
    "import requests\n",
    "from glob import glob\n",
    "from shutil import copyfileobj\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import imageio\n",
    "import numpy as np\n",
    "import zarr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f15fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url and checksum for the data\n",
    "mitoem_url = \"https://www.dropbox.com/sh/p5xn9e4gderjtm6/AADfUMzAA38XBvcXDTG1kAGGa/MitoEM-H.zip?dl=1\"\n",
    "checksum = \"f4ad14e098697be78d3ea13f263f76d5ba81a27e354c9edc906adfe728c765bd\"\n",
    "\n",
    "# where to save the data\n",
    "# the filepaths are for running in google colab, modify if you run this notebook locally\n",
    "data_root = \"./mito-em-data\"  # TODO colab path\n",
    "tmp_path = \"./mito-em-tmp\"\n",
    "\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "os.makedirs(tmp_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41e4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the data is already fully downloaded and initial volume has been created\n",
    "# if yes, we skip the following cells\n",
    "data_path = os.path.join(data_root, \"MitoEM-H.ome.zarr\")\n",
    "have_mitoem_vol = os.path.exists(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af6addf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download https://www.dropbox.com/sh/p5xn9e4gderjtm6/AADfUMzAA38XBvcXDTG1kAGGa/MitoEM-H.zip?dl=1 to ./mito-em-tmp/MitoEM-H.zip: 100%|██████████| 13.8G/13.8G [14:23<00:00, 17.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download to ./mito-em-tmp/MitoEM-H.zip was successful.\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "zip_path = os.path.join(tmp_path, \"MitoEM-H.zip\")\n",
    "if not have_mitoem_vol and not os.path.exists(zip_path):\n",
    "    with requests.get(mitoem_url, stream=True) as r:\n",
    "        filesize = int(r.headers.get(\"Content-Length\", 0))\n",
    "        desc = f\"Download {mitoem_url} to {zip_path}\"\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=filesize, desc=desc) as r_raw, open(zip_path, \"wb\") as f:\n",
    "            copyfileobj(r_raw, f)\n",
    "    with open(zip_path, \"rb\") as f:\n",
    "        this_checksum = hashlib.sha256(f.read()).hexdigest()\n",
    "    if this_checksum == checksum:\n",
    "        print(\"Download to\", zip_path, \"was successful.\")\n",
    "    else:\n",
    "        print(\"The file was downloaded to\", zip_path, \"but the file is likely corrupted!\")\n",
    "        print(\"Please remove\", zip_path, \"and try the download again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b50b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the data\n",
    "unzipped_path = os.path.join(tmp_path, \"MitoEM-H\")\n",
    "if not have_mitoem_vol and not os.path.exists(unzipped_path):\n",
    "    with ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(path=tmp_path)\n",
    "\n",
    "# the file paths for all the pngs with image data\n",
    "image_paths = glob(os.path.join(unzipped_path, \"im\", \"*.png\"))\n",
    "image_paths.sort()\n",
    "# we only write the last 500 image tiles, which correspond to the MitoEM test data-set\n",
    "image_paths = image_paths[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ome.zarr volume from 1000 slices\n"
     ]
    }
   ],
   "source": [
    "# copy the data into a ome.zarr file\n",
    "# TODO actually use ome.zarr instead of normal zarr\n",
    "# FIXME this is much more inefficient than in-memory, need a better solution\n",
    "if not have_mitoem_vol:\n",
    "    # we use dask to copy the data into the ome.zarr array lazily\n",
    "    # see https://docs.dask.org/en/stable/array-creation.html\n",
    "    # https://docs.dask.org/en/latest/generated/dask.array.to_zarr.html\n",
    "    imread = dask.delayed(imageio.imread, pure=True)\n",
    "    print(\"Creating ome.zarr volume from\", len(image_paths), \"slices\")\n",
    "    images = [imread(path) for path in image_paths]\n",
    "    \n",
    "    # find the shape and datatype from the first image\n",
    "    sample = images[0].compute()\n",
    "\n",
    "    # load individual images to get a dask array for each image and stack to get a volume\n",
    "    arrays = [da.from_delayed(im, dtype=sample.dtype, shape=sample.shape)\n",
    "              for im in images]\n",
    "    volume = da.stack(arrays, axis=0)\n",
    "    chunks = (16, 128, 128)\n",
    "    volume.rechunk(chunks)\n",
    "\n",
    "    # write to zarr\n",
    "    da.to_zarr(volume, os.path.join(data_path, \"s0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e997cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [02:47<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write volume to ./mito-em-data/MitoEM-H.ome.zarr\n"
     ]
    }
   ],
   "source": [
    "# copy the data into a ome.zarr file (in memory; implement an efficient version with dask instead)\n",
    "# TODO actually use ome.zarr instead of normal zarr\n",
    "if not have_mitoem_vol:\n",
    "    import z5py\n",
    "    from tqdm import trange\n",
    "    \n",
    "    im0 = imageio.imread(image_paths[0])\n",
    "    shape = (len(image_paths),) + im0.shape\n",
    "    volume = np.zeros(shape, dtype=im0.dtype)\n",
    "    volume[0] = im0\n",
    "    \n",
    "    for z in trange(1, len(image_paths)):\n",
    "        volume[z] = imageio.imread(image_paths[z])\n",
    "    \n",
    "    print(\"Write volume to\", data_path)\n",
    "    chunks = (16, 128, 128)\n",
    "    with z5py.File(data_path, \"a\") as f:\n",
    "        f.create_dataset(\"s0\", data=volume, compression=\"gzip\", n_threads=4, chunks=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d2b33",
   "metadata": {},
   "source": [
    "## Downscale and crop the data\n",
    "\n",
    "Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead2e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use native ome.zarr code as well\n",
    "# make scale pytramid\n",
    "from numcodecs import GZip\n",
    "scale_factors = [[1, 2, 2], [1, 2, 2], [2, 2, 2], [2, 2, 2]]\n",
    "store = zarr.DirectoryStore(data_path)\n",
    "arrays, pyramid = [], []\n",
    "current_factor = np.array([1, 1, 1])\n",
    "with zarr.open(store, mode=\"a\") as f:\n",
    "    data = da.from_zarr(f[\"s0\"])\n",
    "    for scale, factor in enumerate(scale_factors, 1):\n",
    "        current_factor *= np.array(factor)\n",
    "        factor_dict = {k: v for k, v in enumerate(current_factor)}\n",
    "        pyramid.append(\n",
    "            da.coarsen(np.mean, data, factor_dict, trim_excess=True).rechunk(chunks)\n",
    "        )\n",
    "        arrays.append(\n",
    "            f.zeros(name=f\"s{scale}\", shape=pyramid[-1].shape, chunks=chunks, compressor=GZip())\n",
    "        )\n",
    "    da.store(pyramid, arrays, lock=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138506f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a central crop\n",
    "cropped_path = os.path.join(data_root, \"MitoEM-H-cropped.ome.zarr\")\n",
    "store_cropped = zarr.DirectoryStore(cropped_path)\n",
    "crop_shape = (20, 1024, 1024)\n",
    "crop = tuple(slice(sh // 2 - csh // 2, sh // 2 + csh // 2) for sh, csh in zip(data.shape, crop_shape))\n",
    "with zarr.open(store) as f_in, zarr.open(store_cropped, mode=\"a\") as f_out:\n",
    "    f_out.create_dataset(name=\"s0\", data=f_in[\"s0\"][crop], chunks=chunks, compression=GZip())\n",
    "    for scale, factor in enumerate(scale_factors, 1):\n",
    "        crop = tuple(slice(c.start // sf, c.stop // sf) for c, sf in zip(crop, factor))\n",
    "        f_out.create_dataset(name=f\"s{scale}\", data=f_in[f\"s{scale}\"][crop], chunks=chunks, compression=GZip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "designed-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conversion to other formats (if necessary):\n",
    "# export crops for the different resolutions to tifs\n",
    "with zarr.open(store_cropped, \"r\") as f:\n",
    "    for scale in range(len(f)):\n",
    "        data = f[f\"s{scale}\"][:]\n",
    "        imageio.volsave(f\"mito-em-cropped-s{scale}.tif\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
